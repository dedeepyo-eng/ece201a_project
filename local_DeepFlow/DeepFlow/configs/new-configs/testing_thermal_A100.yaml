model_param:
    batch_size: 64
    vocab_size: 40000
    num_layers: 1
    layer_size: 4096
    projection: 2048
    seq_len: 10
    num_gates: 4
    num_non_linear: 5
    num_add: 8
    data_scale: 100 

sw_param:
    kernel_launch_overhead: 9e-6 #9us
    precision: 2 

tech_param:
    core:
        nominal_power_per_mcu: 0.26 #W
        nominal_flop_rate_per_mcu: 500
        nominal_area_per_mcu: 1.01 #mm^2
        nominal_frequency: 1.41e9 #Hz
        nominal_voltage: 0.8 
        threshold_voltage: 0.2 
        margin_voltage: 0.35
        operating_area_per_mcu: 1.01 #!!!!operating_area n nominal_area should chnage hand-in-hand as they affect area scaling
        num_mcu_per_bundle: 4 #a bundle can be thought of as an SM, number of tensorcores per SM
        FMA_d1: 8            # tensor core can compute the results for an d1 x d2 x d1 mixed-precision matrix multiplication per clock
        FMA_d2: 4
        dataflow: "best"        #{wst. ast, ost, best, none}: wst: weight stationary, ast: activation stationary, ost: output stationary
        util: 0.85 #util should be 0.75 (12/16) for tensorcore
    DRAM:
        size: 80 GB
        bandwidth: 7944 GB

        dynamic_energy_per_bit: 4.2e-12 # 0.005e-12 # 0.62e-12 # 1.3e-12 #Joules # 4.2, 8 ### BW # 7944 GB # 1454 GB # 1986 GB # 1.94 TB/s # 1986, 1454
        static_power_per_bit: 0.6e-12
        area_per_bit: 11.54e-10 #mm2
        stack_capacity: 16 GB # 5 HB2Me stacks of 16GB, 8-Hi memory, the updated A100 gets a total of 80GB of memory
        area_per_stack: 86 #mm2
        latency: 100e-9 # 100e-9 # 100, 171
        mem_ctrl_area: 1.1 #mm2 
        nominal_frequency: 3.02e9 # 12.08e9 # 6.04e9 # Twice the bitwidth and twice the frequency. # 12.08e9 # 3.02e9 # Should be double the memory clock frequency, as it is DDR for HBM2E in A100. Same for V100 also. # Even for V100 and A100, operating frequency is higher than twice memory clock frequency.
        nominal_voltage: 1.2 
        threshold_voltage: 0.4 
        margin_voltage: 0.6
        max_voltage: 1.8
        num_links_per_mm: 800 # 400 # 400
        num_links_per_stack: 2048 # 1024 # Each link is 1 bit. Memory bus is 5120 bits wide in A100 80GB per GPU (i.e., 5 HBMs). # Can be said to be number of IO TSVs for data transfer.
        util: 1
    SRAM-L2:
        size: 80 MB
        bandwidth: 7050 GB # source: GTC 2021

        dynamic_energy_per_bit: 130e-15
        static_power_per_bit: 8.4e-12
        area_per_bit: 14.5e-8 # 2.7e-8 # 6.4e-8 #mm2
        bank_capacity: 32 KB
        controller_area_per_link: 0.00 # 0.004 #mm2 # DON'T CARE
        controller_power_per_link: 0.04 #W
        latency: 0
        overhead: 0.20 #20% circuitry overhead in cell area
        util: 1
    SRAM-L1:
        size: 20 MB # max configurable shared memory per SM * number of SMs
        bandwidth: 18 TB

        dynamic_energy_per_bit: 130e-15
        static_power_per_bit: 9e-12
        area_per_bit: 14.1e-8 # 2.7e-8 # 8e-8 #mm2
        bank_capacity: 32 KB
        controller_area_per_link: 0.00 #mm2 
        controller_power_per_link: 0.0 #W
        latency: 0
        overhead: 0.20
        util: 1
    SRAM-R:
        size: 27 MB
        bandwidth : 122 TB

        dynamic_energy_per_bit: 110e-15
        static_power_per_bit: 9e-12
        area_per_bit: 12.78e-8 # 2.7e-8 # 8e-8 #mm2
        bank_capacity: 16 KB # Shouldn't it be 125KB?
        controller_area_per_link: 0.00 #mm2 
        controller_power_per_link: 0.0 #W
        latency: 0
        overhead: 0.25 # 0.25 # overhead should change hand in hand with area_breakdown.L1 (or L2 or reg_mem) : eg. 30 / (1 - 0.25) = 40
        util: 1 
    network:
        intra_node:
          latency: 5e-6 
          nominal_frequency: 12.48e9 # 4.2e9 # 40e9 # 12.5e9 # 
          nominal_voltage: 1
          nominal_energy_per_link: 2e-13 # 8e-12
          nominal_area_per_link: 1e-12 #mm^2
          num_links_per_mm: 7.89 # 3.363
          threshold_voltage: 0.25
          margin_voltage: 0.45
          util: 0.95
        inter_node:
          latency: 5e-6 
          nominal_frequency: 12.48e9
          nominal_voltage: 1
          nominal_energy_per_link: 8e-12
          nominal_area_per_link: 1e-12 #mm^2
          num_links_per_mm: 3.363
          threshold_voltage: 0.25
          margin_voltage: 0.45
          util: 0.95
area_breakdown: # no longer treated as fraction
    device_area_budget: 1260 #mm2
    proc_chip_area_budget: 826 #mm2
    core: 437
    L2: 180
    L1: 30.00
    reg_mem: 41.36
    DRAM: 7
    network:
      intra_node: 66.50 # 0
      inter_node: 66.50 # 133.00

power_breakdown: # no longer treated as fraction #TODO
    TDP: 300
    core: 112.35 #0.4927 if 0.171W per core #0.0189 for 15.7Tflops #0.1504 for 125TFLOps
    DRAM: 25
    L2: 32.12 # Assuming L2 is 80MB and 0.4W per MB, latter same as Volta
    L1: 31.23
    reg_mem: 147.55 # 90.66
    network:
      intra_node: 60.00 # 0.0
      inter_node: 0.00 # 2.805

perimeter_breakdown: # still fraction
    DRAM: 0.36
    intra_node: 0.64 # 0.0
    inter_node: 0.0 # 0.5

system_hierarchy:
    num_devices_per_node: 49 # 25 # it was 1 before as the base config
    num_nodes: 1        # it was 1 before
    inter_derate: 1
    intra_derate: 1
    kp1_inter: False
    kp2_inter: False
    dp_inter: False
    lp_inter: False

network_topology:
    inter_node: "mesh" #mesh, torus, crossbar, custom, hierarchical
    intra_node: "crossbar"

memory_hierarchy:
    l0: #Register Memory
      type: "SRAM-R"
      scope: "mcu"
    l1: #Shared Memory
      type: "SRAM-L1"
      scope: "mcu-bundle"
    l2: #L2 
      type: "SRAM-L2"
      scope: "global"
    l3: #Global Memory
      type: "DRAM"
      scope: "global" 

scheduling_param:
    auto: False 
    dp: 1
    lp: 1
    kp_hidden_dim1: 7
    kp_hidden_dim2: 7
    kp_softmax_dim1: 7
    kp_softmax_dim2: 7
    kp_embedding_dim1: 7
    kp_embedding_dim2: 7
    kp_projection_dim1: 7
    kp_projection_dim2: 7
    kp_hidden_type: 2
    kp_softmax_type: 2
    kp_embedding_type: 2
    kp_projection_type: 2